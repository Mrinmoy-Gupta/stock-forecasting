{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685698444552
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Workspace\n",
        "from azureml.core.environment import EnvironmentReference\n",
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685698444669
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "credential = DefaultAzureCredential()\n",
        "config = json.load(open(\"config.json\"))\n",
        "\n",
        "ml_client = MLClient(\n",
        "    credential = credential,\n",
        "    subscription_id=config['subscription_id'],\n",
        "    resource_group_name=config['resource_group'],\n",
        "    workspace_name=config['workspace_name']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685698445439
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "ws = Workspace.from_config()\n",
        "deployment_env = EnvironmentReference(name='').get_environment(workspace=ws)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%writefile components/production.py\n",
        "import time\n",
        "import http.client, json\n",
        "from datetime import datetime, timedelta\n",
        "from dateutil import parser\n",
        "import dateutil.relativedelta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pytz\n",
        "import argparse\n",
        "import logging\n",
        "import mlflow\n",
        "import os\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from azureml.core import Workspace, Datastore, Dataset\n",
        "from azureml.data.datapath import DataPath\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def datetotimestamp(date):\n",
        "    time_tuple = date.timetuple()\n",
        "    timestamp = round(time.mktime(time_tuple))\n",
        "    return timestamp\n",
        "\n",
        "def timestamptodate(timestamp):\n",
        "    return datetime.fromtimestamp(timestamp)\n",
        "\n",
        "\n",
        "def realTimeData(datastore):\n",
        "\n",
        "    try:\n",
        "        dataset = Dataset.Tabular.from_delimited_files(path=(datastore, 'FILE NAME'))\n",
        "        print(\"++++DATA IS ALREADY PRESENT ++++\")\n",
        "        pandas_df = dataset.to_pandas_dataframe() \n",
        "        result = pandas_df.sort_values(by=\"t\")\n",
        "        temp = result.iloc[-1].tolist()\n",
        "        start = pd.to_datetime(temp[0])\n",
        "        end = datetime.today() + pd.Timedelta(hours=5.5)\n",
        "        c = end - start\n",
        "        minutes = c.total_seconds() // 60\n",
        "        count_back = int(minutes)\n",
        "        end_date = datetotimestamp(end)\n",
        "        start_date = datetotimestamp(start)\n",
        "        url1 = ''\n",
        "        conn = http.client.HTTPSConnection(\"\")\n",
        "        payload = \"\"\n",
        "        headers = {}\n",
        "        conn.request(\"GET\", url1, payload, headers)\n",
        "        res = conn.getresponse()\n",
        "        data = res.read()\n",
        "        response = json.loads(data.decode(\"utf-8\"))\n",
        "        actual_df = pd.DataFrame(response)\n",
        "        actual_df.drop(['s', 'o','h','l','v'], axis=1,inplace=True)\n",
        "        actual_df[\"t\"] = actual_df[\"t\"].apply(timestamptodate)\n",
        "        actual_df[\"t\"] = actual_df[\"t\"] + pd.Timedelta(hours=5.5)\n",
        "        actual_df=actual_df.drop_duplicates('t',keep='first')\n",
        "        df = pd.concat([result, actual_df], ignore_index=False)\n",
        "        print(actual_df.tail())\n",
        "    return df\n",
        "    except:\n",
        "        print(\"****NO PREVIOUS DATA OF STOCK FOUND, COLLECTING DATA FOR ENTIRE YEAR****\")\n",
        "        date = datetime.today() + pd.Timedelta(hours=5.5)\n",
        "        print(date)\n",
        "        prev_yr_date = date + dateutil.relativedelta.relativedelta(months=-12)\n",
        "        print(prev_yr_date)\n",
        "        start = datetotimestamp(prev_yr_date)\n",
        "        end = datetotimestamp(date)\n",
        "        c = date - prev_yr_date\n",
        "        minutes = c.total_seconds() // 60\n",
        "        count_back = int(minutes)\n",
        "        url1 = ''\n",
        "        conn = http.client.HTTPSConnection(\"\")\n",
        "        payload = \"\"\n",
        "        headers = {}\n",
        "        conn.request(\"GET\", url1, payload, headers)\n",
        "        res = conn.getresponse()\n",
        "        data = res.read()\n",
        "        response = json.loads(data.decode(\"utf-8\"))\n",
        "        actual_df = pd.DataFrame(response)\n",
        "        actual_df[\"t\"] = actual_df[\"t\"].apply(timestamptodate)\n",
        "        actual_df[\"t\"] = actual_df[\"t\"] + pd.Timedelta(hours=5.5)\n",
        "        actual_df.drop(['s', 'o','h','l','v'], axis=1,inplace=True)\n",
        "        return actual_df\n",
        "        print(actual_df.tail())\n",
        "    \n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--predictions\", type=str, help=\"path to forecasts\")\n",
        "    parser.add_argument(\"--refined_data\", type=str, help=\"path to refined_data\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    ws = Workspace.get(name=\"\",\n",
        "               subscription_id='',\n",
        "               resource_group='')\n",
        "    datastore = Datastore.get(ws, 'workspaceblobstore')\n",
        "\n",
        "    df = realTimeData(datastore)\n",
        "    now = datetime.today() + pd.Timedelta(hours = 5.5)\n",
        "    now = pd.to_datetime(now.strftime(\"%Y-%m-%d %H:%M\") + \":00\")\n",
        "    print(now)\n",
        "    while(True):\n",
        "        if(now.time()>pd.to_datetime(\"15:30:00\").time()):\n",
        "            break\n",
        "        last = df.iloc[-1].to_list()[1]\n",
        "        if(last == now):\n",
        "            \n",
        "            break\n",
        "        else:\n",
        "            df = realTimeData(datastore)\n",
        "            continue\n",
        "\n",
        "    df.to_csv(os.path.join(args.refined_data, 'refined_data.csv'), index=False)\n",
        "    ds = Dataset.File.upload_directory(src_dir=args.refined_data,\n",
        "            target=DataPath(datastore,  'dataset/mrinmoy/refined_data'),\n",
        "            show_progress=True, overwrite=True)\n",
        "\n",
        "    url = 'http://127.0.0.1:8000/predict'\n",
        "\n",
        "    data = {\n",
        "    'smoothing_trend' : 0.06999999999999999, \n",
        "    'smoothing_seasonal' : 0.05,\n",
        "    'smoothing_level' :  0.39,\n",
        "    }\n",
        "\n",
        "    load = json.dumps(data)\n",
        "    response = requests.post(url, data=load)\n",
        "    response_json = json.loads(response.json())\n",
        "    predictions = pd.DataFrame(response_json['data'], columns=response_json['columns'])\n",
        "    predictions['t'] = pd.to_datetime(predictions['t'])\n",
        "    t = pd.to_datetime(predictions['t'].values[0])\n",
        "    c = predictions['predictions'].values[0]\n",
        "\n",
        "    try:\n",
        "        dataset = Dataset.Tabular.from_delimited_files(path = (datastore, 'dataset/mrinmoy/preds/predictions.csv'))\n",
        "        print(1)\n",
        "        pred_df = dataset.to_pandas_dataframe() \n",
        "        pred_df.drop(columns=pred_df.columns[0], axis=1, inplace=True)\n",
        "        pred_df.loc[len(pred_df)] = t, c\n",
        "    except:\n",
        "        print(2)\n",
        "        pred_df = pd.DataFrame({'t':t, 'c': c}, index = pd.RangeIndex(start=0, step=1, stop=1))\n",
        "    print(pred_df)\n",
        "    \n",
        "    pred_df.to_csv(os.path.join(args.predictions, 'predictions.csv'), index=False)\n",
        "    \n",
        "\n",
        "    ds = Dataset.File.upload_directory(src_dir=args.predictions,\n",
        "            target=DataPath(datastore,  'dataset/mrinmoy/preds'),\n",
        "            show_progress=True, overwrite=True)\n",
        "\n",
        "    \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685702253483
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input, Output\n",
        "\n",
        "deployment_component = command(\n",
        "    name=\"deployment_pipeline\",\n",
        "    display_name=\"Production pipeline for hwes\",\n",
        "    \n",
        "    outputs=dict(\n",
        "        predictions=Output(type=\"uri_folder\", mode=\"rw_mount\", path='./preds/'),\n",
        "        refined_data=Output(type=\"uri_folder\", mode=\"rw_mount\", path='./refined_data/'),\n",
        "    ),\n",
        "    \n",
        "    code= './components/',\n",
        "    command=\"\"\"python production.py \\\n",
        "            --predictions ${{outputs.predictions}} --refined_data ${{outputs.refined_data}}\\\n",
        "            \"\"\",\n",
        "    environment=f\"{deployment_env.name}:{deployment_env.version}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685702254567
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "deployment_component = ml_client.create_or_update(deployment_component.component)\n",
        "\n",
        "print(\n",
        "    f\"Component {deployment_component.name} with Version {deployment_component.version} is registered\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685702254698
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import dsl, Input, Output\n",
        "\n",
        "@dsl.pipeline(\n",
        "    name='deployement_pipeline',\n",
        "    compute='',\n",
        "    description=\"Deployment Pipeline\",\n",
        ")\n",
        "def deployment_pipeline_hwes():\n",
        "    \n",
        "    deployment_job = deployment_component()\n",
        "    \n",
        "    return {\n",
        "        \"predictions\": deployment_job.outputs.predictions,\n",
        "        \"refined_data\": deployment_job.outputs.refined_data\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685702260426
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "pipeline = deployment_pipeline_hwes()\n",
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline,\n",
        "    experiment_name=\"prod-deployment-hwes\",\n",
        ")\n",
        "ml_client.jobs.stream(pipeline_job.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
