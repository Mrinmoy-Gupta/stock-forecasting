{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685697427672
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "\n",
        "credential = DefaultAzureCredential()\n",
        "\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id='',\n",
        "    resource_group_name='',\n",
        "    workspace_name='',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685697427997
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "os.makedirs('./dependencies', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%writefile dependencies/conda.yaml\n",
        "name: model-env\n",
        "channels:\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.8\n",
        "  - numpy=1.21.2\n",
        "  - pip=21.2.4\n",
        "  - scikit-learn=0.24.2\n",
        "  - scipy=1.7.1\n",
        "  - pandas>=1.1,<1.2\n",
        "  - pip:\n",
        "    - inference-schema[numpy-support]==1.3.0\n",
        "    - xlrd==2.0.1\n",
        "    - mlflow== 1.26.1\n",
        "    - azureml-mlflow==1.42.0\n",
        "    - statsmodels==0.12.0\n",
        "    - patsy\n",
        "    - azure-core\n",
        "    - arm-mango\n",
        "    - azureml-dataprep\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685697754714
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "\n",
        "pipeline_job_env = Environment(\n",
        "    name=\"env-hwes\",\n",
        "    description=\"Env for HWES\",\n",
        "    conda_file=\"./dependencies/conda.yaml\",\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
        ")\n",
        "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
        "\n",
        "print(\n",
        "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685697755432
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "os.makedirs('./components',exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%writefile components/data_gather.py\n",
        "import time\n",
        "import http.client, json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from dateutil import parser\n",
        "import dateutil.relativedelta\n",
        "import numpy as np\n",
        "import os\n",
        "from pytz import timezone \n",
        "import argparse\n",
        "import logging\n",
        "import mlflow\n",
        "from azureml.core import Workspace, Datastore, Dataset\n",
        "from azureml.data.datapath import DataPath\n",
        "\n",
        "def datetotimestamp(date):\n",
        "    time_tuple = date.timetuple()\n",
        "    timestamp = round(time.mktime(time_tuple))\n",
        "    return timestamp\n",
        "\n",
        "def timestamptodate(timestamp):\n",
        "    return datetime.fromtimestamp(timestamp)\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--output_data\", type=str, help=\"path to output data data\")\n",
        "    \n",
        "    args = parser.parse_args()\n",
        "    print('output path......', args.output_data)\n",
        "    ws = Workspace.get(name=\"\",\n",
        "               subscription_id='',\n",
        "               resource_group='')\n",
        "\n",
        "    datastore = Datastore.get(ws, 'workspaceblobstore')\n",
        "\n",
        "    try:\n",
        "        dataset = Dataset.Tabular.from_delimited_files(path=(datastore, 'dataset/mrinmoy/data.csv'))\n",
        "        print(\"++++DATA IS ALREADY PRESENT ++++\")\n",
        "        pandas_df = dataset.to_pandas_dataframe() \n",
        "        result = pandas_df.sort_values(by=\"t\")\n",
        "        temp = result.iloc[-1].tolist()\n",
        "        start = temp[1]\n",
        "        end = datetime.today() + pd.Timedelta(hours=5.5)\n",
        "        c = end - start\n",
        "        minutes = c.total_seconds() // 60\n",
        "        count_back = int(minutes)\n",
        "        end_date = datetotimestamp(end)\n",
        "        start_date = datetotimestamp(start)\n",
        "        url1 = ''\n",
        "        conn = http.client.HTTPSConnection(\"\")\n",
        "        payload = \"\"\n",
        "        headers = {}\n",
        "        conn.request(\"GET\", url1, payload, headers)\n",
        "        res = conn.getresponse()\n",
        "        data = res.read()\n",
        "        response = json.loads(data.decode(\"utf-8\"))\n",
        "        actual_df = pd.DataFrame(response)\n",
        "        actual_df.drop(['s', 'o','h','l','v'], axis=1,inplace=True)\n",
        "        actual_df[\"t\"] = actual_df[\"t\"].apply(timestamptodate)\n",
        "        actual_df[\"t\"] = actual_df[\"t\"] + pd.Timedelta(hours=5.5)\n",
        "        actual_df=actual_df.drop_duplicates('t',keep='first')\n",
        "        result = result.drop(columns=['Column1'])\n",
        "        df = pd.concat([result, actual_df], ignore_index=False)\n",
        "        df.to_csv(os.path.join(args.output_data, 'data.csv'), index=False)\n",
        "        print(df)\n",
        "\n",
        "    except:\n",
        "        print(\"****NO PREVIOUS DATA OF STOCK FOUND, COLLECTING DATA FOR ENTIRE YEAR****\")\n",
        "        date = datetime.today() + pd.Timedelta(hours=5.5)\n",
        "        print(date)\n",
        "        prev_yr_date = date + dateutil.relativedelta.relativedelta(months=-12)\n",
        "        print(prev_yr_date)\n",
        "        start = datetotimestamp(prev_yr_date)\n",
        "        end = datetotimestamp(date)\n",
        "        c = date - prev_yr_date\n",
        "        minutes = c.total_seconds() // 60\n",
        "        count_back = int(minutes)\n",
        "        url1 = ''\n",
        "        conn = http.client.HTTPSConnection(\"\")\n",
        "        payload = \"\"\n",
        "        headers = {}\n",
        "        conn.request(\"GET\", url1, payload, headers)\n",
        "        res = conn.getresponse()\n",
        "        data = res.read()\n",
        "        response = json.loads(data.decode(\"utf-8\"))\n",
        "        actual_df = pd.DataFrame(response)\n",
        "        actual_df[\"t\"] = actual_df[\"t\"].apply(timestamptodate)\n",
        "        actual_df[\"t\"] = actual_df[\"t\"] + pd.Timedelta(hours=5.5)\n",
        "        actual_df.drop(['s', 'o','h','l','v'], axis=1,inplace=True)\n",
        "        actual_df.to_csv(os.path.join(args.output_data,'data.csv'), index=False)\n",
        "        print(actual_df)\n",
        "\n",
        "        \n",
        "    ds = Dataset.File.upload_directory(src_dir= args.output_data,\n",
        "            target=DataPath(datastore,  'dataset/mrinmoy'),\n",
        "            show_progress=True, overwrite=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685697755873
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input, Output\n",
        "\n",
        "data_gather_component = command(\n",
        "    name=\"data_gather\",\n",
        "    display_name=\"data gather\",\n",
        "    description=\"fetch data\",\n",
        "    outputs=dict(\n",
        "        output_data=Output(type=\"uri_folder\", mode=\"rw_mount\", path='./data/')\n",
        "    ),\n",
        "    # The source folder of the component\n",
        "    code= './components/',\n",
        "    command=\"\"\"python data_gather.py \\\n",
        "            --output_data ${{outputs.output_data}}\\\n",
        "            \"\"\",\n",
        "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685697756418
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "data_gather_component = ml_client.create_or_update(data_gather_component.component)\n",
        "\n",
        "print(\n",
        "    f\"Component {data_gather_component.name} with Version {data_gather_component.version} is registered\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%writefile components/data_prep.py\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import argparse\n",
        "import logging\n",
        "import mlflow\n",
        "from datetime import datetime, timedelta\n",
        "from dateutil import parser\n",
        "import dateutil.relativedelta\n",
        "import pytz\n",
        "from azureml.core import Workspace, Datastore, Dataset\n",
        "from azureml.data.datapath import DataPath\n",
        "\n",
        "def data_process(start, end, df):\n",
        "    duplicate_df = pd.date_range(start,end, freq='T')\n",
        "    df_temp = pd.DataFrame({ 't': duplicate_df, 'd': None }) \n",
        "    common = df_temp.merge(df, on=[\"t\"])\n",
        "    row_not_in_originaldf = df_temp[~df_temp.t.isin(common.t)]\n",
        "    row_not_in_originaldf.reset_index(drop=True, inplace=True)\n",
        "    row_not_in_originaldf['c'] = row_not_in_originaldf['d']\n",
        "    new_row_not_in_originaldf=row_not_in_originaldf.drop('d',axis=1)\n",
        "    final_df=pd.concat([df,new_row_not_in_originaldf],ignore_index=True)\n",
        "    final_df = final_df.sort_values(by=\"t\",ignore_index=True)\n",
        "    final_df=final_df.drop_duplicates('t',keep='first')\n",
        "    new_final_df=final_df.fillna(method='ffill')\n",
        "    new_final_df.drop(new_final_df.index[-1], inplace=True)\n",
        "    return new_final_df\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data\", type=str, help=\"path to input data\")\n",
        "    parser.add_argument(\"--n_test_points\", type=int, required=False, default=300)\n",
        "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
        "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
        "    parser.add_argument(\"--refined_data\", type=str, help=\"path to refined data\")\n",
        "    \n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Start Logging\n",
        "    mlflow.start_run()\n",
        "\n",
        "    df = pd.read_csv(os.path.join(args.data, 'data.csv'))\n",
        "    df=df.sort_values(by='t')\n",
        "\n",
        "    df['t'] = pd.to_datetime(df['t'])\n",
        "    df['t'] = df['t'] + pd.Timedelta(hours=5.5)\n",
        "    \n",
        "    start_date=df.iloc[0].tolist()\n",
        "    end_date=df.iloc[-1].tolist()\n",
        "    new_final_df = data_process(start_date[0], end_date[0], df)\n",
        "    print()\n",
        "    print(new_final_df)\n",
        "    print()\n",
        "    train_df = new_final_df.iloc[:-args.n_test_points]\n",
        "    test_df = new_final_df.iloc[-args.n_test_points:]\n",
        "\n",
        "    mlflow.log_metric(\"num_train_samples\", train_df.shape[0])\n",
        "    mlflow.log_metric(\"num_test_samples\", test_df.shape[0])\n",
        "\n",
        "    print(train_df.shape)\n",
        "    print(test_df.shape)\n",
        "\n",
        "    train_path = os.path.join(args.train_data, 'train.csv')\n",
        "    test_path = os.path.join(args.test_data, 'test.csv')\n",
        "    \n",
        "    print(train_path)\n",
        "    print(test_path)\n",
        "\n",
        "    train_df.to_csv(train_path, index=False)\n",
        "    test_df.to_csv(test_path, index=False)\n",
        "\n",
        "    now = datetime.today() + pd.Timedelta(hours = 5.5)\n",
        "    end = pd.to_datetime(now.strftime(\"%Y-%m-%d\") + \" 09:15:00\")\n",
        "#     end = now\n",
        "    print(end)\n",
        "    new_df = data_process(start_date[0], end, df)\n",
        "    refined_data_path = os.path.join(args.refined_data, 'refined_data.csv')\n",
        "    print(new_df.tail())\n",
        "    new_df.to_csv(refined_data_path, index=False)\n",
        "\n",
        "    ws = Workspace.get(name=\"\",\n",
        "    subscription_id='',\n",
        "    resource_group='')\n",
        "\n",
        "    datastore = Datastore.get(ws, 'workspaceblobstore')\n",
        "\n",
        "    ds = Dataset.File.upload_directory(src_dir=args.train_data,\n",
        "            target=DataPath(datastore,  'dataset/mrinmoy/train'),\n",
        "            show_progress=True, overwrite=True)\n",
        "    \n",
        "    ds = Dataset.File.upload_directory(src_dir=args.test_data,\n",
        "            target=DataPath(datastore,  'dataset/mrinmoy/test'),\n",
        "            show_progress=True, overwrite=True)\n",
        "\n",
        "    ds = Dataset.File.upload_directory(src_dir=args.refined_data,\n",
        "            target=DataPath(datastore,  'dataset/mrinmoy/refined_data'),\n",
        "            show_progress=True, overwrite=True)\n",
        "\n",
        "    # Stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685697756910
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input, Output\n",
        "\n",
        "data_prep_component = command(\n",
        "    name=\"data_prep_hwes\",\n",
        "    display_name=\"Data preparation for training\",\n",
        "\n",
        "    inputs={\n",
        "        \"data\": Input(type='uri_folder', mode='rw_mount'),\n",
        "        \"n_test_points\": Input(type=\"integer\"),\n",
        "    },\n",
        "\n",
        "    outputs=dict(\n",
        "        train_data=Output(type=\"uri_folder\", mode=\"rw_mount\", path='./data_hwes/'),\n",
        "        test_data=Output(type=\"uri_folder\", mode=\"rw_mount\", path='./data_hwes/'),\n",
        "        refined_data=Output(type=\"uri_folder\", mode=\"rw_mount\", path='./data_hwes/'),\n",
        "    ),\n",
        "    \n",
        "    code= './components/',\n",
        "    command=\"\"\"python data_prep.py \\\n",
        "            --data ${{inputs.data}} --n_test_points ${{inputs.n_test_points}} \\\n",
        "            --train_data ${{outputs.train_data}} --test_data ${{outputs.test_data}} --refined_data ${{outputs.refined_data}}\\\n",
        "            \"\"\",\n",
        "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685697758372
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "data_prep_component = ml_client.create_or_update(data_prep_component.component)\n",
        "\n",
        "print(\n",
        "    f\"Component {data_prep_component.name} with Version {data_prep_component.version} is registered\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685697758531
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(\"./components/train\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%writefile components/train.py\n",
        "import argparse\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "import os\n",
        "from datetime import datetime, timezone\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import mlflow\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.holtwinters import SimpleExpSmoothing   \n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import mango\n",
        "from mango import Tuner\n",
        "\n",
        "# Start Logging\n",
        "\n",
        "\n",
        "os.makedirs(\"./results\", exist_ok=True)\n",
        "\n",
        "mlflow.start_run()\n",
        "\n",
        "def main():\n",
        "    \n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
        "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
        "    parser.add_argument(\"--registered_model_name\", type=str, help=\"model name\")\n",
        "    parser.add_argument(\"--model\", type=str, help=\"path to model\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    train_df = pd.read_csv(os.path.join(args.train_data, 'train.csv'))\n",
        "    train_df['t'] = pd.to_datetime(train_df['t'])\n",
        "    train_df['t'] = train_df['t'].dt.tz_localize(None)\n",
        "    train_df.set_index(\"t\", inplace=True)\n",
        "\n",
        "    test_df = pd.read_csv(os.path.join(args.test_data, 'test.csv'))\n",
        "    test_df['t'] = pd.to_datetime(test_df['t'])\n",
        "    test_df['t'] = test_df['t'].dt.tz_localize(None)\n",
        "    test_df.set_index(\"t\", inplace=True)\n",
        "\n",
        "    print(\"train info\")\n",
        "    print(train_df.info())\n",
        "\n",
        "    print(\"test info\")\n",
        "    print(test_df.info())\n",
        "\n",
        "    def objective_function(args_list):\n",
        "        errors = []\n",
        "        for params in args_list:\n",
        "            try:\n",
        "                model = ExponentialSmoothing(train_df['c'],trend='MUL',seasonal='MUL', freq='T', seasonal_periods=1440).fit(optimized=False, **params)\n",
        "                forecast = model.forecast(len(test_df)).values\n",
        "                error = mse(test_df['c'], forecast, squared=False) \n",
        "                errors.append(error)\n",
        "            except:\n",
        "                errors.append(1000.0)\n",
        "        return errors\n",
        "\n",
        "    param_grid = dict(\n",
        "    smoothing_level = np.linspace(0.01, 0.9, 10),\n",
        "    smoothing_trend =  np.linspace(0.01, 0.9, 10),\n",
        "    smoothing_seasonal = np.linspace(0.01, 0.9, 10),\n",
        "    )\n",
        "    grid = ParameterGrid(param_grid)\n",
        "\n",
        "    conf_Dict = dict()\n",
        "    conf_Dict['initial_random'] = 100\n",
        "    conf_Dict['num_iteration'] = 50\n",
        "\n",
        "    tuner = Tuner(param_grid, objective_function, conf_Dict)\n",
        "    results = tuner.minimize()\n",
        "    print(\"HyperParameter Tuning completed!\")\n",
        "\n",
        "\n",
        "    print('smoothing_trend', results['best_params']['smoothing_trend'])\n",
        "    print('smoothing_seasonal', results['best_params']['smoothing_seasonal'])\n",
        "    print('smoothing_level', results['best_params']['smoothing_level'])\n",
        "\n",
        "    mlflow.log_param('smoothing_trend', results['best_params']['smoothing_trend'])\n",
        "    mlflow.log_param('smoothing_seasonal', results['best_params']['smoothing_seasonal'])\n",
        "    mlflow.log_param('smoothing_level', results['best_params']['smoothing_level'])\n",
        "\n",
        "    df_train_temp = train_df.copy()\n",
        "    forecasts = np.array([])\n",
        "    for i in range(len(test_df)):\n",
        "        fitted_model = ExponentialSmoothing(df_train_temp['c'],trend='MUL',seasonal='MUL', freq='T',\n",
        "                                            seasonal_periods=1440).fit(\n",
        "                                            smoothing_trend = results['best_params']['smoothing_trend'], \n",
        "                                            smoothing_seasonal = results['best_params']['smoothing_seasonal'],\n",
        "                                            smoothing_level =  results['best_params']['smoothing_level'],\n",
        "                                            optimized=False)\n",
        "        forecast = fitted_model.forecast(1).values[0]\n",
        "        forecasts = np.append(forecasts, forecast)\n",
        "        df_train_temp.reset_index(inplace=True)\n",
        "        df_train_temp.loc[len(df_train_temp)] = test_df.index[i], forecast\n",
        "        df_train_temp.set_index(\"t\", inplace=True)\n",
        "    df_predictions = df_train_temp.iloc[-len(test_df):]\n",
        "    rmse = mse(test_df['c'], df_predictions, squared=False)\n",
        "\n",
        "    mlflow.log_metric('RMSE', rmse)\n",
        "    \n",
        "    train = pd.concat([train_df, test_df], ignore_index=False)\n",
        "    print(train.info())\n",
        "    print()\n",
        "    print(train)\n",
        "    fitted_model = ExponentialSmoothing(train['c'],trend='MUL',seasonal='MUL', freq='T',\n",
        "                                            seasonal_periods=1440).fit(\n",
        "                                            smoothing_trend = results['best_params']['smoothing_trend'], \n",
        "                                            smoothing_seasonal = results['best_params']['smoothing_seasonal'],\n",
        "                                            smoothing_level =  results['best_params']['smoothing_level'],\n",
        "                                            optimized=False)\n",
        "\n",
        "    print(\"saving model with mlflow\")\n",
        "    mlflow.statsmodels.save_model(\n",
        "        statsmodels_model=fitted_model,\n",
        "        path = args.model\n",
        "    )\n",
        "\n",
        "    mlflow.end_run()\n",
        "   \n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685697758823
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input, Output\n",
        "\n",
        "train_component = command(\n",
        "    name=\"train_hwes\",\n",
        "    display_name=\"Train HWES\",\n",
        "    description=\"Finds Hyper patameter for HWES\",\n",
        "    inputs={\n",
        "        \"train_data\": Input(type=\"uri_folder\"),\n",
        "        \"test_data\": Input(type=\"uri_folder\"),\n",
        "        \"registered_model_name\": Input(type=\"string\")\n",
        "    },\n",
        "    outputs=dict(\n",
        "        model= Output(type='uri_folder', mode='rw_mount', path='./results/')\n",
        "    ),\n",
        "    code=\"./components/\",\n",
        "    command=\"\"\"python train.py \\\n",
        "            --train_data ${{inputs.train_data}} --test_data ${{inputs.test_data}} --registered_model_name ${{inputs.registered_model_name}} \\\n",
        "            --model ${{outputs.model}} \\\n",
        "            \"\"\",\n",
        "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685697759422
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "train_component = ml_client.create_or_update(train_component.component)\n",
        "\n",
        "print(\n",
        "    f\"Component {train_component.name} with Version {train_component.version} is registered\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685697759576
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import dsl, Input, Output\n",
        "\n",
        "\n",
        "@dsl.pipeline(\n",
        "    compute='',\n",
        "    description=\"Training pipeline for HWES\",\n",
        ")\n",
        "def pipeline_hwes(\n",
        "    pipeline_job_n_test_points,\n",
        "    pipeline_job_train_data,\n",
        "    pipeline_job_test_data,\n",
        "    pipeline_job_refined_data,\n",
        "    pipeline_job_registered_model_name,\n",
        "\n",
        "\n",
        "):\n",
        "\n",
        "    data_gather_job = data_gather_component()\n",
        "    \n",
        "    data_prep_job = data_prep_component(\n",
        "        data = data_gather_job.outputs.output_data,\n",
        "        n_test_points=pipeline_job_n_test_points\n",
        "    )\n",
        "\n",
        "    train_job = train_component(\n",
        "        train_data=data_prep_job.outputs.train_data,  \n",
        "        test_data=data_prep_job.outputs.test_data,  \n",
        "        registered_model_name = pipeline_job_registered_model_name,\n",
        "        )\n",
        "\n",
        "    # a pipeline returns a dictionary of outputs\n",
        "    # keys will code for the pipeline output identifier\n",
        "    return {\n",
        "        \"pipeline_job_train_data\": data_prep_job.outputs.train_data,\n",
        "        \"pipeline_job_test_data\": data_prep_job.outputs.test_data,\n",
        "        \"pipeline_job_refined_data\": data_prep_job.outputs.refined_data,\n",
        "        \"pipeline_job_model\":train_job.outputs.model\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685697759759
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "pipeline = pipeline_hwes(\n",
        "    pipeline_job_n_test_points=5,\n",
        "    pipeline_job_registered_model_name='holts-winter-model',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685697965463
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline,\n",
        "    experiment_name=\"pipeline-hwes\",\n",
        ")\n",
        "ml_client.jobs.stream(pipeline_job.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
